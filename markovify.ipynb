{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba6ca9de",
   "metadata": {},
   "source": [
    "Hello user! Here is a little guide on how to use this code:\n",
    "\n",
    "To create a sequence that only has within-sequence Markovian dependencies, run the first section. If you instead have an input that you want to have a response to, run the second section. You probably don't need to run the third section, it's old and I kept it only as a reference, just in case.\n",
    "\n",
    "1. Make sure this notebook and the PrettyDat.csv file are in the same folder.\n",
    " \n",
    "2. Whenever you are restarting this notebook, run the 'GLOBAL' cell at the top of this document to install the dependencies. \n",
    "\n",
    "3. The first time you are running this code, you will have to run the 'GENERATE CORPORA' cell make the corpus text files for each species from the PrettyDat.csv file that accompanied this notebook. This cell will save the .txt files to the same folder that this notebook and the PrettyDat.csv file are in.\n",
    "\n",
    "4. **Only edit the User Input section of the next cell.** I've added some annotations there to help you know what each variable means.\n",
    "\n",
    "5. Run the cell! That's it, easy peasy :) The generated output will be printed below the chunk.\n",
    "\n",
    "-Ananya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311e7bd7",
   "metadata": {},
   "source": [
    "## 2nd Order Markov Generation for Single Fish, no interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffbbcc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## GLOBAL ########\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0db64e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## GENERATE CORPORA ############\n",
    "PrettyDat = pd.read_csv('PrettyDat.csv')\n",
    "\n",
    "def create_corpus_by_species(df, by_length=True):\n",
    "    \"\"\"\n",
    "    Create dom/sub/full corpora split by species.\n",
    "    \n",
    "    Args:\n",
    "        df: dataframe with columns ['species','trial_id','sequence_id','fish_id','order_id','state','length']\n",
    "        out_prefix: prefix for output files\n",
    "        by_length: whether to expand states by 'length'\n",
    "    \"\"\"\n",
    "    # Loop over each species\n",
    "    for species, species_df in df.groupby('species'):\n",
    "        dom_filename  = f\"corpus_{species}_dom.txt\"\n",
    "        sub_filename  = f\"corpus_{species}_sub.txt\"\n",
    "        full_filename = f\"corpus_{species}_full.txt\"\n",
    "\n",
    "        grouped = species_df.groupby(['trial_id', 'sequence_id'])\n",
    "\n",
    "        with open(dom_filename, 'w') as f, open(sub_filename, 'w') as g, open(full_filename, 'w') as h:\n",
    "            for (trial_id, seq_id), group in grouped:\n",
    "                # Process dom then sub\n",
    "                for idx, fish_role in enumerate(['dom', 'sub']):\n",
    "                    role_group = group[group['fish_id'] == fish_role].sort_values('order_id')\n",
    "                    sequence_parts = []\n",
    "                    for _, row in role_group.iterrows():\n",
    "                        # Repeat state by_length\n",
    "                        if by_length:\n",
    "                            chars = [str(row['state'])] * int(row['length'])\n",
    "                        else:\n",
    "                            chars = [str(row['state'])]\n",
    "\n",
    "                        sequence_parts.append(''.join(chars))\n",
    "\n",
    "                    sequence_line = \"<s>\" + ''.join(sequence_parts) + '</s>\\n'\n",
    "\n",
    "                    if idx == 0:\n",
    "                        f.write(sequence_line)\n",
    "                    else:\n",
    "                        g.write(sequence_line)\n",
    "\n",
    "                    h.write(sequence_line)\n",
    "\n",
    "#create corpora by species and role, save to same folder\n",
    "create_corpus_by_species(PrettyDat, by_length=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc35d4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: aaaaaaaaaaaaaaaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaaaaaaaaaaaaaaaaaaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaaaaaaaaaaaaaaaaaaaaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaaaaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaaaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaccccccccccccccccccccccccccccceeeeeeaaaaaaaaaaaaaaaaaaeeeeeeeaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaahhhhhhhhhaccccccccccccccccccccccccccccccccccccccccccccccccccccccccceeeeeeeeeeeeeeeccccccccccccccccccccccaaaaaaaaaaaaa\n"
     ]
    }
   ],
   "source": [
    "############## USER INPUT #####################\n",
    "species = 'bre' #choose from 'bre', 'mil', 'mul', 'oce', 'orn', 'pul'\n",
    "type = 'dom' #choose what corpus to reference for conditionals: 'dom' (dom only corpus), 'sub' (sub only corpus), 'full' (dom and sub corpora combined)\n",
    "start_pair = ('a', 'e') #choose starting pair of states for generation\n",
    "steps = 30 #how many state transitions to generate\n",
    "############ NOW JUST RUN :) ##################\n",
    "\n",
    "### helper functions: ###\n",
    "def read_lines(filename):\n",
    "    \"\"\"Read corpus by line\"\"\"\n",
    "    lines = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # Remove <s> and </s>\n",
    "            line = line.replace(\"<s>\", \"\").replace(\"</s>\", \"\").strip()\n",
    "            seq = list(line)\n",
    "            lines.append(seq)\n",
    "    return lines\n",
    "\n",
    "def find_length_dists(lines):\n",
    "    \"\"\"\n",
    "    For each state, find distribution of lengths.\n",
    "    Returns:\n",
    "      - length_dists: dict[state][length] = probabilities\n",
    "      - df_long: tidy DataFrame with columns [state, length, prob], for reference\n",
    "    \"\"\"\n",
    "    lengths = defaultdict(Counter)\n",
    "    \n",
    "    for seq in lines:\n",
    "        run_char, run_len = seq[0], 1\n",
    "        for s in seq[1:]:\n",
    "            if s == run_char:\n",
    "                run_len += 1\n",
    "            else:\n",
    "                lengths[run_char][run_len] += 1\n",
    "                run_char, run_len = s, 1\n",
    "        lengths[run_char][run_len] += 1 \n",
    "    \n",
    "    # normalize\n",
    "    length_dists = {}\n",
    "    tidy_rows = []\n",
    "    for state, counter in lengths.items():\n",
    "        total = sum(counter.values())\n",
    "        length_dists[state] = {l: c / total for l, c in counter.items()}\n",
    "        for l, c in counter.items():\n",
    "            tidy_rows.append({\"state\": state, \"length\": l, \"prob\": c / total})\n",
    "    \n",
    "    #make a handy table for reference \n",
    "    df_long = pd.DataFrame(tidy_rows)\n",
    "\n",
    "    return length_dists, df_long\n",
    "\n",
    "def collapse_length(seq):\n",
    "    \"\"\"Collapse length to find 2nd order Markov dependencies.\"\"\"\n",
    "    collapsed = []\n",
    "    for s in seq:\n",
    "        if not collapsed or collapsed[-1] != s:\n",
    "            collapsed.append(s)\n",
    "    return collapsed\n",
    "\n",
    "def find_transitions(lines):\n",
    "    \"\"\"\n",
    "    Build transitions based on the last two unique states.\n",
    "    Returns dict: (prev_state, last_state) -> {next_state: prob}\n",
    "    \"\"\"\n",
    "    transitions = defaultdict(Counter)\n",
    "    \n",
    "    for seq in lines:\n",
    "        collapsed = collapse_length(seq)\n",
    "        for t in range(2, len(collapsed)):\n",
    "            prev2, prev1, curr = collapsed[t-2], collapsed[t-1], collapsed[t]\n",
    "            transitions[(prev2, prev1)][curr] += 1\n",
    "    \n",
    "    # Normalize\n",
    "    normalized = {}\n",
    "    for cond, counter in transitions.items():\n",
    "        total = sum(counter.values())\n",
    "        normalized[cond] = {s: c / total for s, c in counter.items()}\n",
    "    return normalized\n",
    "\n",
    "def sample_from_dist(dist):\n",
    "    \"\"\"Sample from distribution, weighted by probability\"\"\"\n",
    "    states, probs = zip(*dist.items())\n",
    "    return random.choices(states, weights=probs, k=1)[0]\n",
    "\n",
    "def sim_2nd_ord_markov(trans, length_dists, start_pair, steps):\n",
    "    \"\"\"\n",
    "    Simulate sequence with unique-state transitions AND sampled lengths.\n",
    "    start_pair: tuple (state1, state2)\n",
    "    Returns expanded sequence with durations.\n",
    "    \"\"\"\n",
    "    seq_states = [start_pair[0], start_pair[1]]\n",
    "    expanded_seq = []\n",
    "    \n",
    "    # expand start_pair with sampled lengths\n",
    "    for state in seq_states:\n",
    "        if state in length_dists:\n",
    "            dur = sample_from_dist(length_dists[state])\n",
    "            expanded_seq.extend([state] * dur)\n",
    "        else:\n",
    "            expanded_seq.append(state)\n",
    "    \n",
    "    for t in range(2, steps):\n",
    "        cond = (seq_states[-2], seq_states[-1])\n",
    "        if cond in trans:\n",
    "            next_state = sample_from_dist(trans[cond])\n",
    "        else:\n",
    "            next_state = random.choice(list({s for _, v in trans.items() for s in v}))\n",
    "        \n",
    "        seq_states.append(next_state)\n",
    "        \n",
    "        # expand using sampled length\n",
    "        if next_state in length_dists:\n",
    "            dur = sample_from_dist(length_dists[next_state])\n",
    "            expanded_seq.extend([next_state] * dur)\n",
    "        else:\n",
    "            expanded_seq.append(next_state)\n",
    "    \n",
    "    return expanded_seq\n",
    "\n",
    "### generation: ###\n",
    "lines = read_lines(f\"corpus_{species}_{type}.txt\") #read corpus\n",
    "\n",
    "trans = find_transitions(lines) #record two-state transitions\n",
    "\n",
    "length_dists, df_long = find_length_dists(lines) #record length distributions per state\n",
    "\n",
    "# generate! :)\n",
    "sim_seq = sim_2nd_ord_markov(trans, length_dists, start_pair = start_pair, steps = steps)\n",
    "print(\"Generated:\", \"\".join(sim_seq))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bba84c",
   "metadata": {},
   "source": [
    "## A little experiment: 2 state markovian generation within random markovian environment\n",
    "\n",
    "**Let D be a dominant sequence in one trial, and S be the subordinate sequence in the same trial. D and S are random variables with Markov chain properties:**\n",
    "\n",
    "$$P(d_t|d_1,...,d_{t-1}) = P(d_t | d_{t-1})$$\n",
    "\n",
    "$$P(s_t|s_1,...,s_{t-1}) = P(s_t | s_{t-1})$$\n",
    "\n",
    "**But D and S are also somehow dependent on each other, since there is feedback between fish during communication:**\n",
    "\n",
    "Let: \n",
    "\n",
    "$r_1$ = the dominant's reaction time\n",
    "\n",
    "$r_2$ = the subordinate's reaction time\n",
    "\n",
    "$$P(d_t|d_1,...,d_{t-1}) = P(d_t|d_{t-1}, s_{t-r_{1}})$$\n",
    "\n",
    "$$P(s_t|s_1,...,s_{t-1}) = P(s_t|s_{t-1}, d_{t-r_{2}})$$\n",
    "\n",
    "**Furthermore, from my R analyses there seems to be evidence of higher-order Markov dependencies within D and S separately (2nd-order at least)**\n",
    "\n",
    "Let:\n",
    "\n",
    "D = dominant sequence with length\n",
    "\n",
    "S = subordinate sequence with length\n",
    "\n",
    "D' = dominant sequence collapsed by length\n",
    "\n",
    "S' = subordinate sequence collapsed by length\n",
    "\n",
    "$r_1$ = the dominant's reaction time\n",
    "\n",
    "$r_2$ = the subordinate's reaction time\n",
    "\n",
    "$$P(d_t|d_1,...,d_{t-1}) = P(d_t|d'_{t-1}, d'_{t-2}, s_{t-r_{1}})$$\n",
    "\n",
    "$$P(s_t|s_1,...,s_{t-1}) = P(s_t|s'_{t-1}, s'_{t-2}, d_{t-r_{2}})$$\n",
    "\n",
    "\n",
    "\n",
    "**Thoughts:**\n",
    "\n",
    "While the physical postures of fish are quite likely to be Markovian, the *meaning* behind postures may not necessarily be structured in a Markovian way. This means that the 'responses' between fish may not be Markov-related...Therefore, it is not so valid to assume that the relationship between D and S is Markovian, even if they are separately Markov chains.... This makes the assumption that their reactions to each other are sequential\n",
    "\n",
    "but what else to do?\n",
    "\n",
    "Have set both fish's reaction times to 1 frame, around 1ms which is what most research seems to say (although it is a bit unclear... I would guess they are actually a bit faster than this, but this is the highest resolution that we can do since the camera is only 60fps)\n",
    "\n",
    "\n",
    "- maybe to do: look at number of combinations in 1st, 2nd, 3rd, etc. order markovs--if there is underlying higher structure/motifs, the number of transitions should plateau at some point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfcf9b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeehhhhhhhhhheeeeeeeeeeeeeecccccccccccccccccccddddddddddddddddddddddddddddddddddddddddddddddddddddddeeeeeeeeeeeeeeeeeeeeeeeeeeeaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaccccchhhhhhcccchhhhhhhcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccckkkhhhcccccccccccccccccccccccccccchhhhhhccccceeeeeeeeeeehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhccccccccccccccccccccccccbbbbbbcccccgggggggcccffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffgggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggfffcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccffffffffffeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaaaaaaaaaaaaaaaaaaaaaaaaaaaaaccccccccccccjjjjjjjjjjjjjjjjjjjjjjjffffffffffffffffffffffffffffffffffffeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeccccccccccccccccccddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddjjjjjjjjjjaaaaaaaaaaaaaaaaaaaaaaaajjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjdddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddcccccccccccccccdddddddddddddddddfffggggfffffffffffffffffffffffccccccccccccccccccccccccccccccccccccccccccccccccccccccccceeeeeeeeeefffggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggfffbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbggggggggggggggggggggggffffffffffffeeeeecceeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeccccccccccfffffffffffffffeeeeeeeeeecccccccccfffffffffffffffffffffffffffffffffffffffffffffffffff\n"
     ]
    }
   ],
   "source": [
    "############## USER INPUT #####################\n",
    "species = 'mul' #choose from 'bre', 'mil', 'mul', 'oce', 'orn', 'pul'\n",
    "sub_seq = 'ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggffffffffbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccgggggggggggggghhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhccccccccccccccccccccccccccggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccgggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggghhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhkkkkkkkkkkkkkkkkkkkkkkkkkkcccccccccccccddddddddddddddddddddddddddddddddddddddddddddddddjjjjjjjjjjjjjjjjjjjjjjaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaaaaaaaaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaaaaaaaaaaaaaaaaaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee'\n",
    "start_pair = ('a', 'e') #choose starting pair of states for generation\n",
    "steps = 20 #how many states to generate (for coupled markov only)\n",
    "############ NOW JUST RUN :) ##################\n",
    "\n",
    "### helper functions: ###\n",
    "#read lines from both corpora\n",
    "def load_both_lines(dom_file, sub_file):\n",
    "    \"\"\"Read dom/sub corpora as aligned lists of sequences (per line).\"\"\"\n",
    "\n",
    "    dom_lines = []\n",
    "    sub_lines = []\n",
    "    with open(dom_file) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # Remove <s> and </s>\n",
    "            line = line.replace(\"<s>\", \"\").replace(\"</s>\", \"\").strip()\n",
    "            seq = list(line)\n",
    "            dom_lines.append(seq)\n",
    "        #dom_lines = [line.strip().replace(\"<s>\", \"\").replace(\"</s>\", \"\").split() for line in f]\n",
    "    with open(sub_file) as g:\n",
    "        for line in g:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # Remove <s> and </s>\n",
    "            line = line.replace(\"<s>\", \"\").replace(\"</s>\", \"\").strip()\n",
    "            seq = list(line)\n",
    "            sub_lines.append(seq)\n",
    "        #sub_lines = [line.strip().replace(\"<s>\", \"\").replace(\"</s>\", \"\").split() for line in g]\n",
    "    \n",
    "    # Ensure same number of sequences\n",
    "    assert len(dom_lines) == len(sub_lines), \"mismatched sequence numbers :(\"\n",
    "    return dom_lines, sub_lines\n",
    "\n",
    "def find_length_dists(lines):\n",
    "    \"\"\"\n",
    "    For each state, find distribution of lengths.\n",
    "    Returns:\n",
    "      - length_dists: dict[state][length] = probabilities\n",
    "      - df_long: tidy DataFrame with columns [state, length, prob], for reference\n",
    "    \"\"\"\n",
    "    lengths = defaultdict(Counter)\n",
    "    \n",
    "    for seq in lines:\n",
    "        run_char, run_len = seq[0], 1\n",
    "        for s in seq[1:]:\n",
    "            if s == run_char:\n",
    "                run_len += 1\n",
    "            else:\n",
    "                lengths[run_char][run_len] += 1\n",
    "                run_char, run_len = s, 1\n",
    "        lengths[run_char][run_len] += 1 \n",
    "    \n",
    "    # normalize\n",
    "    length_dists = {}\n",
    "    tidy_rows = []\n",
    "    for state, counter in lengths.items():\n",
    "        total = sum(counter.values())\n",
    "        length_dists[state] = {l: c / total for l, c in counter.items()}\n",
    "        for l, c in counter.items():\n",
    "            tidy_rows.append({\"state\": state, \"length\": l, \"prob\": c / total})\n",
    "    \n",
    "    # make a handy table for reference \n",
    "    df_long = pd.DataFrame(tidy_rows)\n",
    "\n",
    "    return length_dists, df_long\n",
    "\n",
    "def collapse_length(seq):\n",
    "    \"\"\"Collapse length to find 2nd order Markov dependencies of d'/s' sequences.\"\"\"\n",
    "    collapsed = []\n",
    "    for s in seq:\n",
    "        if not collapsed or collapsed[-1] != s:\n",
    "            collapsed.append(s)\n",
    "    return collapsed\n",
    "\n",
    "def coupled_transitions(dom_lines, sub_lines):\n",
    "    \"\"\"\n",
    "    Build coupled 2nd-order Markov transitions:\n",
    "      Dom: P(d_t | d'_{t-2}, d'_{t-1}, s_{t-1})\n",
    "      Sub: P(s_t | s'_{t-2}, s'_{t-1}, d_{t-1})\n",
    "    \"\"\"\n",
    "\n",
    "    dom_transitions = defaultdict(Counter)\n",
    "    sub_transitions = defaultdict(Counter)\n",
    "    \n",
    "    for dom_seq, sub_seq in zip(dom_lines, sub_lines):\n",
    "        # Collapse runs for state-based dependencies\n",
    "        collapsed_dom = collapse_length(dom_seq)\n",
    "        collapsed_sub = collapse_length(sub_seq)\n",
    "\n",
    "        # need to align collapsed indices with token indices\n",
    "        # Build maps from collapsed index → token index (last token of each run)\n",
    "        dom_run_ends = []\n",
    "        sub_run_ends = []\n",
    "        \n",
    "        # indices where each collapsed run ends\n",
    "        run_char = dom_seq[0]\n",
    "        for i, s in enumerate(dom_seq[1:], start=1):\n",
    "            if s != run_char:\n",
    "                dom_run_ends.append(i-1)\n",
    "                run_char = s\n",
    "        dom_run_ends.append(len(dom_seq)-1)\n",
    "        \n",
    "        run_char = sub_seq[0]\n",
    "        for i, s in enumerate(sub_seq[1:], start=1):\n",
    "            if s != run_char:\n",
    "                sub_run_ends.append(i-1)\n",
    "                run_char = s\n",
    "        sub_run_ends.append(len(sub_seq)-1)\n",
    "        \n",
    "        L = min(len(collapsed_dom), len(collapsed_sub))\n",
    "        \n",
    "        # Iterate through collapsed states\n",
    "        for t in range(2, L):\n",
    "            # dom conditional\n",
    "            cond_dom = (\n",
    "                collapsed_dom[t-2],           # d'_{t-2}\n",
    "                collapsed_dom[t-1],           # d'_{t-1}\n",
    "                sub_seq[dom_run_ends[t-1]]    # s_{t-1} = last token of that sub run\n",
    "            )\n",
    "            dom_next = collapsed_dom[t]\n",
    "            dom_transitions[cond_dom][dom_next] += 1\n",
    "\n",
    "            # sub conditional\n",
    "            cond_sub = (\n",
    "                collapsed_sub[t-2],           # s'_{t-2}\n",
    "                collapsed_sub[t-1],           # s'_{t-1}\n",
    "                dom_seq[sub_run_ends[t-1]]    # d_{t-1} = last token of that dom run\n",
    "            )\n",
    "            sub_next = collapsed_sub[t]\n",
    "            sub_transitions[cond_sub][sub_next] += 1\n",
    "    \n",
    "    # Normalize into probabilities\n",
    "    def normalize(trans):\n",
    "        return {\n",
    "            cond: {k: v / sum(c.values()) for k, v in c.items()} \n",
    "            for cond, c in trans.items()\n",
    "        }\n",
    "    \n",
    "    return normalize(dom_transitions), normalize(sub_transitions)\n",
    "\n",
    "def sample_from_dist(dist):\n",
    "    states, probs = zip(*dist.items())\n",
    "    return random.choices(states, weights=probs, k=1)[0]\n",
    "\n",
    "def simulate_coupled_markov(dom_trans, sub_trans,\n",
    "                            dom_length_dists, sub_length_dists,\n",
    "                            start_pair, steps=20):\n",
    "    \"\"\"\n",
    "    Jointly simulate Dom and Sub with:\n",
    "      Dom ~ P(d_t | d'_{t-2}, d'_{t-1}, s_{t-1})\n",
    "      Sub ~ P(s_t | s'_{t-2}, s'_{t-1}, d_{t-1})\n",
    "    \"\"\"\n",
    "\n",
    "    # Track collapsed states\n",
    "    dom_states = [start_pair[0], start_pair[1]]\n",
    "    sub_states = [start_pair[0], start_pair[1]]\n",
    "\n",
    "    # Track expanded tokens\n",
    "    dom_expanded = []\n",
    "    sub_expanded = []\n",
    "\n",
    "    # Expand initial states\n",
    "    for state in dom_states:\n",
    "        dur = sample_from_dist(dom_length_dists[state])\n",
    "        dom_expanded.extend([state] * dur)\n",
    "    for state in sub_states:\n",
    "        dur = sample_from_dist(sub_length_dists[state])\n",
    "        sub_expanded.extend([state] * dur)\n",
    "\n",
    "    # Generate subsequent states\n",
    "    for t in range(2, steps):\n",
    "        # dom update\n",
    "        sub_prev_token = sub_expanded[-1]  # last sub *token*\n",
    "        cond_dom = (dom_states[-2], dom_states[-1], sub_prev_token)\n",
    "        if cond_dom in dom_trans:\n",
    "            dom_next = sample_from_dist(dom_trans[cond_dom])\n",
    "        else:\n",
    "            dom_next = random.choice(list({s for dist in dom_trans.values() for s in dist}))\n",
    "        dom_states.append(dom_next)\n",
    "\n",
    "        dur = sample_from_dist(dom_length_dists[dom_next])\n",
    "        dom_expanded.extend([dom_next] * dur)\n",
    "\n",
    "        #sub update\n",
    "        dom_prev_token = dom_expanded[-1]  # last dom *token*\n",
    "        cond_sub = (sub_states[-2], sub_states[-1], dom_prev_token)\n",
    "        if cond_sub in sub_trans:\n",
    "            sub_next = sample_from_dist(sub_trans[cond_sub])\n",
    "        else:\n",
    "            sub_next = random.choice(list({s for dist in sub_trans.values() for s in dist}))\n",
    "        sub_states.append(sub_next)\n",
    "\n",
    "        dur = sample_from_dist(sub_length_dists[sub_next])\n",
    "        sub_expanded.extend([sub_next] * dur)\n",
    "\n",
    "    return dom_expanded, sub_expanded\n",
    "\n",
    "def generate_dom_given_sub(sub_seq, dom_trans, dom_length_dists, start_states):\n",
    "    \"\"\"\n",
    "    Generate Dom sequence conditioned on full Sub sequence.\n",
    "    Uses P(d_t | d'_{t-2}, d'_{t-1}, s_{t-1}).\n",
    "    \"\"\"\n",
    "    dom_states = [start_states[0], start_states[1]] \n",
    "    dom_expanded = []\n",
    "\n",
    "    # Expand initial states\n",
    "    for state in dom_states:\n",
    "        dur = sample_from_dist(dom_length_dists[state])\n",
    "        dom_expanded.extend([state] * dur)\n",
    "\n",
    "    for t in range(1, len(sub_seq)):\n",
    "        sub_prev_token = sub_seq[t-1]\n",
    "\n",
    "        cond = (dom_states[-2], dom_states[-1], sub_prev_token)\n",
    "        if cond in dom_trans:\n",
    "            dom_next = sample_from_dist(dom_trans[cond])\n",
    "        else:\n",
    "            dom_next = random.choice(list({s for dist in dom_trans.values() for s in dist}))\n",
    "        dom_states.append(dom_next)\n",
    "\n",
    "        dur = sample_from_dist(dom_length_dists[dom_next])  \n",
    "        dom_expanded.extend([dom_next] * dur)\n",
    "\n",
    "        if len(dom_expanded) >= len(sub_seq):\n",
    "            dom_expanded = dom_expanded[:len(sub_seq)]\n",
    "            break\n",
    "\n",
    "    return dom_expanded\n",
    "\n",
    "### generation: ###\n",
    "dom_lines, sub_lines = load_both_lines(f\"corpus_{species}_dom.txt\", f\"corpus_{species}_sub.txt\")\n",
    "\n",
    "dom_trans, sub_trans = coupled_transitions(dom_lines, sub_lines)\n",
    "\n",
    "dom_length_dists, dom_df_long = find_length_dists(dom_lines)\n",
    "sub_length_dists, sub_df_long = find_length_dists(sub_lines)\n",
    "\n",
    "#coupled markov (both fish jointly generated, basically the computer talking to itself)\n",
    "#you probably won't need to use this\n",
    "#dom_seq, sub_seq = simulate_coupled_markov(dom_trans, sub_trans,\n",
    "#                            dom_length_dists, sub_length_dists,\n",
    "#                            start_pair, steps= steps)\n",
    "\n",
    "#print(\"Dom:\", \"\".join(dom_seq))\n",
    "#print(\"Sub:\", \"\".join(sub_seq))\n",
    "\n",
    "#response to input sequence (one fish response generated to given input sequence)\n",
    "dom_seq = generate_dom_given_sub(sub_seq, dom_trans, dom_length_dists, start_pair)\n",
    "\n",
    "print(\"Generated:\", \"\".join(dom_seq))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74045cd",
   "metadata": {},
   "source": [
    "## Old, Don't Use: 1st-Order Markov Generation for Single Fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76d2fc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eeeeeeehccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccchhhhhhhhhhhhhhhhhhhkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdddddddcccccccccccccccccccccccddddddddddddddddddddddddddfffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffgggggggggggggggccccccckkkkkkkkkkkkkkkkkkkkkkkkkkccccccccccccccccccccccccccccccceeeeeeeeeeeeeeeeeeeaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaeeeacaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaacccccccccccccaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeecccccccccccccccccccccccccccccccccccccccccccccccccccckkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkccccccccccccccccccaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaacccccc\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_single_corpus_lines(filename):\n",
    "    \"\"\"Read dom/sub corpora as aligned lists of sequences (per line).\"\"\"\n",
    "    lines = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # Remove <s> and </s>\n",
    "            line = line.replace(\"<s>\", \"\").replace(\"</s>\", \"\").strip()\n",
    "            # Treat each character as a state\n",
    "            seq = list(line)\n",
    "            if len(seq) >= 3:  # must have at least 3 for transitions\n",
    "                lines.append(seq)\n",
    "    return lines\n",
    "\n",
    "def single_transitions(lines):\n",
    "    transitions = defaultdict(Counter)\n",
    "    \n",
    "    for seq in lines:\n",
    "        L = len(seq)        \n",
    "        \n",
    "        for t in range(1, L-1):\n",
    "            transitions[seq[t]][seq[t+1]] += 1\n",
    "    \n",
    "    # normalize\n",
    "    def normalize(trans):\n",
    "        return {cond: {k: v/sum(c.values()) for k,v in c.items()} \n",
    "                for cond,c in trans.items()}\n",
    "    \n",
    "    return normalize(transitions)\n",
    "\n",
    "\n",
    "def sample_from_distribution(dist):\n",
    "    states, probs = zip(*dist.items())\n",
    "    return random.choices(states, weights=probs, k=1)[0]\n",
    "\n",
    "def simulate_single(trans, start_state, steps=500):\n",
    "    sim_seq = [start_state]\n",
    "    \n",
    "    for t in range(1, steps):\n",
    "        # Dom depends on dom[t-1], sub[t-1]\n",
    "        cond = sim_seq[-1]\n",
    "        if cond in trans:\n",
    "            next = sample_from_distribution(trans[cond])\n",
    "        else:\n",
    "            next = random.choice(list({k[0] for k in trans}))\n",
    "        sim_seq.append(next)\n",
    "    \n",
    "    return sim_seq\n",
    "\n",
    "#load species\n",
    "lines = load_single_corpus_lines(\"corpus_mul_dom.txt\")\n",
    "\n",
    "# Build transition models\n",
    "trans = single_transitions(lines)\n",
    "\n",
    "#stating state for each fish\n",
    "start_state = 'e'\n",
    "\n",
    "#generate 1000 states\n",
    "test_seq= simulate_single(trans, start_state, steps=1000)\n",
    "print(\"\".join(test_seq))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
